{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('MNIST')\n",
    "X_test, Y_test = mndata.load_testing()\n",
    "X_train, Y_train = mndata.load_training()\n",
    "\n",
    "X_test = np.array(list(map(lambda x: np.array(x), X_test))).T\n",
    "Y_test = np.array(Y_test).T\n",
    "\n",
    "X_train = np.array(list(map(lambda x: np.array(x), X_train))).T\n",
    "Y_train = np.array(Y_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28\n",
    "INPUTS_NUMBER = IMAGE_WIDTH*IMAGE_WIDTH\n",
    "CLASSES_NUMBER = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test):\n",
    "    total = np.concatenate((X_train, X_test), axis=1)\n",
    "    avg = np.mean(total, axis=1, keepdims=True)\n",
    "    norm = np.linalg.norm(total, axis=1, keepdims=True)\n",
    "    norm = np.array([np.apply_along_axis(lambda x: x if x != 0 else 1, arr=norm, axis=1)]).T\n",
    "    X_train_normalized = (X_train - avg)/norm\n",
    "    X_test_normalized = (X_test - avg)/norm\n",
    "    return X_train_normalized, X_test_normalized\n",
    "\n",
    "X_train_normalized, X_test_normalized = normalize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_normalized\n",
    "X_test = X_test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "def one_hot(labels):\n",
    "    onehot = np.zeros((CLASSES_NUMBER, len(labels)))\n",
    "    for i in range(len(labels)):\n",
    "        onehot[int(labels[i]),i] = 1\n",
    "    return onehot\n",
    "\n",
    "Y_train_onehot = one_hot(Y_train)\n",
    "Y_test_onehot = one_hot(Y_test)\n",
    "\n",
    "print(Y_train_onehot.shape)\n",
    "print(Y_test_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = Y_train_onehot\n",
    "Y_test = Y_test_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(10, 60000)\n",
      "(784, 10000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_image(record):\n",
    "    plt.imshow(record.reshape(IMAGE_WIDTH,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    res = 1./(1.+np.exp(-z))\n",
    "    return (res, z)\n",
    "\n",
    "def relu(z):\n",
    "    return (np.maximum(0, z), z)\n",
    "    \n",
    "    \n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    A = sigmoid(Z)[0]\n",
    "    dx = dA*(A*(1-A))\n",
    "    return dx\n",
    "    \n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    z = cache\n",
    "    dz = np.array(dA, copy=True)\n",
    "    dz[z <= 0] = 0\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = []\n",
    "    for i in range(1,len(layers_dims)):\n",
    "        layer_params = {}\n",
    "        layer_params['W'] = tf.Variable(tf.random_normal([layers_dims[i], layers_dims[i-1]]), name='W'+str(i), dtype=tf.float32)\n",
    "        layer_params['b'] = tf.Variable(tf.zeros((layers_dims[i], 1), dtype=tf.float32), name='b'+str(i))\n",
    "        parameters.append(layer_params)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'W': <tf.Variable 'W1_46:0' shape=(5, 4) dtype=float32_ref>,\n",
       "  'b': <tf.Variable 'b1_46:0' shape=(5, 1) dtype=float32_ref>},\n",
       " {'W': <tf.Variable 'W2_27:0' shape=(6, 5) dtype=float32_ref>,\n",
       "  'b': <tf.Variable 'b2_27:0' shape=(6, 1) dtype=float32_ref>}]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters([4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = tf.matmul(W, A) + b\n",
    "    return Z\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b):\n",
    "    Z = linear_forward(A_prev, W, b)\n",
    "    A = tf.nn.relu(Z)        \n",
    "    return A\n",
    "\n",
    "\n",
    "def model(dim, X):\n",
    "    \n",
    "    parameters = initialize_parameters(dim)\n",
    "    \n",
    "    L = len(parameters)\n",
    "    A = X\n",
    "    \n",
    "    for i in range(L-1):\n",
    "        W = parameters[i]['W']\n",
    "        b = parameters[i]['b']\n",
    "        A = linear_activation_forward(A, W, b)\n",
    "    W = parameters[L-1]['W']\n",
    "    b = parameters[L-1]['b']\n",
    "    A = linear_forward(A, W, b)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.20228713e-05]\n",
      " [-1.27018650e-04]\n",
      " [ 2.34575856e-04]\n",
      " [-1.07710890e-04]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float64, [2, None], name='X')\n",
    "mod = model([2,3,4], X)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(mod, feed_dict={X: [[1],[2]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(index, size, X, Y):\n",
    "    begin = index*size\n",
    "    end = index*size+size\n",
    "    end = end if end < X.shape[1] else X.shape[1] - 1\n",
    "    return X[:, begin: end], Y[:, begin: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_ = tf.placeholder(tf.float32, [CLASSES_NUMBER, None])\n",
    "X_ = tf.placeholder(tf.float32, [INPUTS_NUMBER, None])\n",
    "logits = model([INPUTS_NUMBER, 256, 256, CLASSES_NUMBER], X_)\n",
    "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_, dim=0))#tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_, dim=0))#-tf.reduce_sum(Y_*tf.log(recognizer))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  16.924866\n",
      "cost:  2.9364462\n",
      "cost:  3.080306\n",
      "cost:  1.8927426\n",
      "cost:  1.9163662\n",
      "cost:  4.050782\n",
      "cost:  1.5441906\n",
      "cost:  2.5731888\n",
      "cost:  1.7643876\n",
      "cost:  2.2412143\n",
      "cost:  1.5724758\n",
      "cost:  1.5723088\n",
      "cost:  1.1148348\n",
      "cost:  1.9962246\n",
      "cost:  1.0339694\n",
      "cost:  1.1783919\n",
      "cost:  0.88981354\n",
      "cost:  1.0579739\n",
      "cost:  1.7679377\n",
      "cost:  0.95110357\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "batch_idx = 0\n",
    "for i in range(2000):\n",
    "    batch_idx = batch_idx if batch_idx*512 < X_train.shape[1] else 0 \n",
    "    batch_x, batch_y = batch(batch_idx, 512, X_train, Y_train)\n",
    "    batch_idx += 1\n",
    "    _, cost = sess.run([train_step, cost_function], feed_dict={X_: batch_x, Y_: batch_y})\n",
    "    if i%100 == 0:\n",
    "        print(\"cost: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = tf.nn.softmax(logits, dim=0)\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 0), tf.argmax(Y_, 0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7226167\n",
      "0.7278\n"
     ]
    }
   ],
   "source": [
    "# print(logits.eval(feed_dict={X_: X_train, Y_: Y_train}).shape)\n",
    "# print(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_, dim=0).eval(feed_dict={X_: X_train, Y_: Y_train}).shape)\n",
    "# lin\n",
    "print(accuracy.eval(feed_dict={X_: X_train, Y_: Y_train}))\n",
    "print(accuracy.eval(feed_dict={X_: X_test, Y_: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=39.985888077\n",
      "Epoch: 0002 cost=24.718713039\n",
      "Epoch: 0003 cost=17.404589753\n",
      "Epoch: 0004 cost=13.036918770\n",
      "Epoch: 0005 cost=10.550756191\n",
      "Epoch: 0006 cost=8.954216694\n",
      "Epoch: 0007 cost=7.898723516\n",
      "Epoch: 0008 cost=7.098989445\n",
      "Epoch: 0009 cost=6.457547848\n",
      "Epoch: 0010 cost=5.942457181\n",
      "Epoch: 0011 cost=5.531623243\n",
      "Epoch: 0012 cost=5.206263472\n",
      "Epoch: 0013 cost=4.936797193\n",
      "Epoch: 0014 cost=4.699576872\n",
      "Epoch: 0015 cost=4.496543126\n",
      "Optimization Finished!\n",
      "Train Accuracy: 0.73143333\n",
      "Test Accuracy: 0.741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import MNIST data\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [n_input, None])\n",
    "Y = tf.placeholder(\"float\", [n_classes, None])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes, n_hidden_2]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1, 1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2, 1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes, 1]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(weights['h1'],x), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(weights['h2'], layer_1), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(weights['out'], layer_2) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y, dim=0))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(X_train.shape[1]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch(i, batch_size, X_train, Y_train)\n",
    "            \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "                                                            Y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits, dim=0)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.transpose(pred), 1), tf.argmax(tf.transpose(Y), 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=251.133857581\n",
      "Epoch: 0002 cost=100.544263288\n",
      "Epoch: 0003 cost=74.374830352\n",
      "Epoch: 0004 cost=63.270150401\n",
      "Epoch: 0005 cost=53.643589960\n",
      "Epoch: 0006 cost=46.250007435\n",
      "Epoch: 0007 cost=41.650627502\n",
      "Epoch: 0008 cost=38.120960002\n",
      "Epoch: 0009 cost=34.394468255\n",
      "Epoch: 0010 cost=31.718446169\n",
      "Epoch: 0011 cost=28.957364607\n",
      "Epoch: 0012 cost=27.204996125\n",
      "Epoch: 0013 cost=26.170167691\n",
      "Epoch: 0014 cost=24.515232554\n",
      "Epoch: 0015 cost=23.293204926\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.8745\n",
      "Train Accuracy: 0.88567275\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "                                                            Y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    print(\"Train Accuracy:\", accuracy.eval({X: mnist.train.images, Y: mnist.train.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
