{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('MNIST')\n",
    "X_test, Y_test = mndata.load_testing()\n",
    "X_train, Y_train = mndata.load_training()\n",
    "\n",
    "X_test = np.array(list(map(lambda x: np.array(x), X_test))).T\n",
    "Y_test = np.array(Y_test).T\n",
    "\n",
    "X_train = np.array(list(map(lambda x: np.array(x), X_train))).T\n",
    "Y_train = np.array(Y_train).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test):\n",
    "    total = np.concatenate((X_train, X_test), axis=1)\n",
    "    avg = np.mean(total, axis=1, keepdims=True)\n",
    "    norm = np.linalg.norm(total, axis=1, keepdims=True)\n",
    "    norm = np.array([np.apply_along_axis(lambda x: x if x != 0 else 1, arr=norm, axis=1)]).T\n",
    "    X_train_normalized = (X_train - avg)/norm\n",
    "    X_test_normalized = (X_test - avg)/norm\n",
    "    return X_train_normalized, X_test_normalized\n",
    "\n",
    "X_train_normalized, X_test_normalized = normalize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_test_normalized\n",
    "X_test = X_test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "def one_hot(labels):\n",
    "    onehot = np.zeros((CLASSES_NUMBER, len(labels)))\n",
    "    for i in range(len(labels)):\n",
    "        onehot[int(labels[i]),i] = 1\n",
    "    return onehot\n",
    "\n",
    "Y_train_onehot = one_hot(Y_train)\n",
    "Y_test_onehot = one_hot(Y_test)\n",
    "\n",
    "print(Y_train_onehot.shape)\n",
    "print(Y_test_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = Y_test_onehot\n",
    "Y_test = Y_test_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28\n",
    "INPUTS_NUMBER = IMAGE_WIDTH*IMAGE_WIDTH\n",
    "CLASSES_NUMBER = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_image(record):\n",
    "    plt.imshow(record.reshape(IMAGE_WIDTH,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    res = 1./(1.+np.exp(-z))\n",
    "    return (res, z)\n",
    "\n",
    "def relu(z):\n",
    "    return (np.maximum(0, z), z)\n",
    "    \n",
    "    \n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    A = sigmoid(Z)[0]\n",
    "    dx = dA*(A*(1-A))\n",
    "    return dx\n",
    "    \n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    z = cache\n",
    "    dz = np.array(dA, copy=True)\n",
    "    dz[z <= 0] = 0\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = []\n",
    "    for i in range(1,len(layers_dims)):\n",
    "        layer_params = {}\n",
    "        layer_params['W'] = tf.Variable(np.random.randn(layers_dims[i], layers_dims[i-1])*0.01, name='W'+str(i))\n",
    "        layer_params['b'] = tf.Variable(tf.zeros((layers_dims[i], 1), dtype=tf.float64), name='b'+str(i))\n",
    "        parameters.append(layer_params)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'W': <tf.Variable 'W1:0' shape=(5, 4) dtype=float64_ref>,\n",
       "  'b': <tf.Variable 'b1:0' shape=(5, 1) dtype=float64_ref>},\n",
       " {'W': <tf.Variable 'W2:0' shape=(6, 5) dtype=float64_ref>,\n",
       "  'b': <tf.Variable 'b2:0' shape=(6, 1) dtype=float64_ref>}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters([4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Softmax Regression cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    Y_ = tf.placeholder(tf.float64, [None,None])\n",
    "    cost = -tf.reduce_sum(Y_*tf.log(AL))  \n",
    "    with tf.Session() as ses:\n",
    "        ses.run(tf.global_variables_initializer())\n",
    "        return ses.run(cost, feed_dict={Y_: Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.68213122712422\n"
     ]
    }
   ],
   "source": [
    "AL = tf.Variable([[1., 2., 3.]], dtype=tf.float64)\n",
    "\n",
    "print(compute_cost(AL, [[1, 2, 3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = tf.matmul(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b):\n",
    "    Z = linear_forward(A_prev, W, b)\n",
    "    A = tf.nn.relu(Z)        \n",
    "    return A\n",
    "\n",
    "\n",
    "def model(dim, X):\n",
    "    \n",
    "    parameters = initialize_parameters(dim)\n",
    "    \n",
    "    L = len(parameters)\n",
    "    A = X\n",
    "    \n",
    "    for i in range(L-1):\n",
    "        W = parameters[i]['W']\n",
    "        b = parameters[i]['b']\n",
    "        A = linear_activation_forward(A, W, b)\n",
    "    W = parameters[L-1]['W']\n",
    "    b = parameters[L-1]['b']\n",
    "    A = linear_forward(A, W, b)\n",
    "    \n",
    "    return tf.nn.softmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float64, [2, None], name='X')\n",
    "mod = model([2,3,4], X)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(mod, feed_dict={X: [[1],[2]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_ = tf.placeholder(tf.float64, [CLASSES_NUMBER, None])\n",
    "X_ = tf.placeholder(tf.float64, [INPUTS_NUMBER, None])\n",
    "cost_function = -tf.reduce_sum(Y_*tf.log(model([INPUTS_NUMBER, 6, CLASSES_NUMBER], X_)))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def backward_prop(dAL, caches):\n",
    "    L = len(caches)\n",
    "    dA = dAL\n",
    "    grads = []\n",
    "    dA, dW, db = linear_activation_backward(dA, caches[L-1], 'sigmoid')\n",
    "    layer_grads = {'dW': dW, 'db': db}\n",
    "    grads.append(layer_grads)\n",
    "    for i in reversed(range(0, L-1)):\n",
    "        dA, dW, db = linear_activation_backward(dA, caches[i], 'relu')\n",
    "        layer_grads = {'dW': dW, 'db': db}\n",
    "        grads.append(layer_grads)\n",
    "    grads.reverse()\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for i in range(len(parameters)):\n",
    "        parameters[i]['W'] = parameters[i]['W'] - grads[i]['dW']*learning_rate\n",
    "        parameters[i]['b'] = parameters[i]['b'] - grads[i]['db']*learning_rate\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def predict(parameters, X):\n",
    "    probs = forward_prop(parameters=parameters, X=X)[0]\n",
    "    return (probs > 0.5).astype(int)\n",
    "\n",
    " \n",
    "def L_layer_model(X_train, Y_train, X_test, Y_test, layers_dims, num_iterations=2000, batch_size=512, learning_rate=0.05):\n",
    "    np.random.seed(1)\n",
    "    layers_dims.insert(0, X_train.shape[0])\n",
    "    parameters = initialize_parameters(layers_dims=layers_dims)\n",
    "    costs = []\n",
    "    iterations = []\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_prop(parameters=parameters, X=X_train)\n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(AL=AL, Y=Y_train)\n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        dAL = -(np.divide(Y_train, AL) - np.divide(1 - Y_train, 1 - AL))\n",
    "        grads = backward_prop(dAL, caches)\n",
    "        \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(grads=grads, learning_rate=learning_rate, parameters=parameters)\n",
    "        # Print the cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            iterations.append(i)\n",
    "            costs.append(cost)\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    Y_prediction_test = predict(parameters,X=X_test)\n",
    "    Y_prediction_train = predict(parameters,X=X_train)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    plt.plot(iterations, costs)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
